{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d2f548-aeca-4191-a1f8-ba2c29952062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google word embeddings loaded\n",
      "cuda:0\n",
      "Quadro GV100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbcac1825f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as tt\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data_preprocessing as dp\n",
    "import utils\n",
    "from models.ResNet import ResNet\n",
    "import train_zsl\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4ee4df-c73f-44a9-8f56-0800ec8eb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17dcf485-36fa-4df9-b39b-bdbeb7d29c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../TINY_IMGNET/zsl_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c253687-75c8-43ca-904d-e97cd9889e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set images: 75000\n",
      "Validation set images: 7500\n",
      "ZSL set images: 25000\n"
     ]
    }
   ],
   "source": [
    "train_ds, valid_ds, zsl_ds = dp.create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a844409-0a2a-4b58-95de-535479ff327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories split into seen and unseen\n",
      "Labels transformed into average labels\n",
      "Label vectors preprocessed\n",
      "Target vectors normalized\n"
     ]
    }
   ],
   "source": [
    "label_vecs, target_labels, zsl_label_vecs, zsl_target_labels, train_target_vectors_norm = dp.preprocess_labels(train_ds, zsl_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e369f4-fede-4993-8f35-9e0d5b4ad8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6bd83b-131e-49c6-9e0a-a0243b6544c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet(3, 150)\n",
    "net = nn.DataParallel(net, device_ids=['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'])\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596bcca9-fd76-4c71-b241-065e388f1e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./model_weights/CE_2_15_ces_0_55.pth')\n",
    "net.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb68b7d-31d2-4aad-94e1-2716282317a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsl_target_labels_list = list(zsl_target_labels.values())\n",
    "zsl_class_vecs_list = list(zsl_label_vecs.values())\n",
    "zsl_target_vectors = torch.cat(zsl_class_vecs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23cd887d-0b2b-4e83-a43e-077b70af7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsl_loader = DataLoader(zsl_ds, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55f66cd3-252a-464f-9ce5-56c9692df804",
   "metadata": {},
   "outputs": [],
   "source": [
    "zsl_ids = {v: k for k, v in zsl_ds.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be136adc-be2f-483a-8497-70be87413098",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = KDTree(zsl_target_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe8a941-ccc0-4665-9207-3e7564b3f18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels_list = list(target_labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a89fd36c-3326-4e20-847d-ad0bc2646960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google word embeddings loaded\n",
      "Categories split into seen and unseen\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_VECS = dp.load_vectors()\n",
    "train_cat, zsl_cat = dp.split_classes()\n",
    "labels = pd.read_csv('./words.txt', sep='\\t', header=None)\n",
    "train_labels_df = labels[labels[0].isin(train_cat)]\n",
    "zsl_labels_df = labels[labels[0].isin(zsl_cat)]\n",
    "\n",
    "zsl_labels_df['average_label'] = zsl_labels_df[1].transform(dp.average_label)\n",
    "zsl_labels_df['average_vector'] = zsl_labels_df['average_label'].transform(GOOGLE_VECS.get_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d7cfbc-7fde-49e5-862d-ff5d901313f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_zsl_seen(model):\n",
    "    model.eval()\n",
    "    top_5 = []\n",
    "    top_1 = []\n",
    "    start_time = time()\n",
    "\n",
    "    for i, (zsl_img, zsl_target) in enumerate(zsl_loader):\n",
    "        zsl_img = zsl_img.to(device)\n",
    "        zsl_target = zsl_target.to(device)\n",
    "        zsl_x = zsl_img\n",
    "        zsl_target = zsl_target.tolist()\n",
    "\n",
    "        class_id_batch = [zsl_ids[class_num] for class_num in zsl_target] # target class ids from target batch\n",
    "        labels_batch = [zsl_labels_df[zsl_labels_df[0] == class_id]['average_label'].item() for class_id in class_id_batch]\n",
    "\n",
    "        pred_emb = model(zsl_x)[1]\n",
    "        pred_emb = pred_emb.to('cpu')\n",
    "        emb_batch = torch.Tensor.cpu(pred_emb.detach()).numpy().squeeze()\n",
    "\n",
    "        vec_batch = [np.expand_dims(emb, axis=0) for emb in emb_batch]\n",
    "\n",
    "        index_5_batch = [tree.query(vec, k=5, return_distance=False) for vec in vec_batch]\n",
    "        index_5_batch = [arr.squeeze() for arr in index_5_batch]\n",
    "\n",
    "        pred_ids_5 = [[zsl_target_labels_list[index] for index in array] for array in index_5_batch]\n",
    "        pred_labels_5 = [[zsl_labels_df[zsl_labels_df[0] == class_id]['average_label'].item() for class_id in array] for array in pred_ids_5]\n",
    "\n",
    "        index_1_batch = [tree.query(vec, k=1, return_distance=False) for vec in vec_batch]\n",
    "        index_1_batch = [arr.squeeze() for arr in index_1_batch]\n",
    "\n",
    "        pred_ids_1 = [zsl_target_labels_list[index] for index in index_1_batch]\n",
    "        pred_labels_1 = [zsl_labels_df[zsl_labels_df[0] == class_id]['average_label'].item() for class_id in pred_ids_1]\n",
    "\n",
    "        pairs_1 = list(zip(labels_batch, pred_labels_1))\n",
    "        pairs_5 = list(zip(labels_batch, pred_labels_5))\n",
    "\n",
    "        top1 = sum([x[0] == x[1] for x in pairs_1])\n",
    "        top_1.append(top1)\n",
    "        \n",
    "        top5 = 0\n",
    "        for pair in pairs_5:\n",
    "            top = pair[0] in pair[1]\n",
    "            top5 += top\n",
    "        top_5.append(top5)\n",
    "#         print(top1, top5, batch_size)\n",
    "              \n",
    "    top_1_mean = sum(top_1) / len(top_1)\n",
    "    top_5_mean = sum(top_5) / len(top_5)\n",
    "    \n",
    "#     print(top_1_mean, top_5_mean)\n",
    "    \n",
    "    print(f'Top-1 accuracy: {round(top_1_mean / float(batch_size), 4)}')\n",
    "    print(f'Top-5 accuracy: {round(top_5_mean / float(batch_size), 4)}')\n",
    "\n",
    "    compute_time = round((time()-start_time), 2)\n",
    "    print(f'Time: {compute_time} sec \\n')\n",
    "    \n",
    "    return pairs_1, pairs_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b86ecf7-51d4-4a39-a963-9454c4df397b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 0.0778\n",
      "Top-5 accuracy: 0.2585\n",
      "Time: 65.11 sec \n",
      "\n"
     ]
    }
   ],
   "source": [
    "zsl_pairs = evaluate_zsl_seen(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
